#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åˆ†å—å’Œé›¶çŸ¥è¯†è¯æ˜ç”Ÿæˆ
åœ¨ç™»è®°æ•°æ®é›†æ—¶ï¼Œå¦‚æœæ£€æµ‹åˆ°æ°´å°ï¼Œä½¿ç”¨å·²è®¡ç®—çš„æœ€ä¼˜çº¦æŸå‚æ•°(X,M,m)å¯¹ZKè¾“å…¥æ•°æ®è¿›è¡Œåˆ†å—
"""

import os
import sys
import json
import logging
import numpy as np
import shutil
from typing import Dict, List, Any, Optional

# æ·»åŠ featuresç›®å½•åˆ°path
sys.path.append(os.path.join(os.path.dirname(__file__)))

from addWatermark import find_zk_input_for_buy_hash

class Stage3ChunkProcessor:
    """ç¬¬ä¸‰é˜¶æ®µæ•°æ®åˆ†å—å¤„ç†å™¨"""
    
    def __init__(self, data_dir=None):
        """
        åˆå§‹åŒ–åˆ†å—å¤„ç†å™¨
        
        Args:
            data_dir: æ•°æ®ç›®å½•ï¼Œé»˜è®¤ä¸º../data
        """
        if data_dir is None:
            self.data_dir = os.path.join(os.path.dirname(__file__), "..", "data")
        else:
            self.data_dir = data_dir
        
        self.zk_inputs_dir = os.path.join(self.data_dir, "zk_inputs")
        self.chunked_inputs_dir = os.path.join(self.data_dir, "chunked_inputs")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.chunked_inputs_dir, exist_ok=True)
        
        logging.info(f"Stage3ChunkProcessoråˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")
    
    def chunk_zk_input_data(self, buy_hash: str, chunk_pixel_size: int, 
                           output_session_dir: str = None) -> Dict[str, Any]:
        """
        æ ¹æ®chunk_pixel_sizeå¯¹ZKè¾“å…¥æ•°æ®è¿›è¡Œåˆ†å—
        å­¦ä¹ LSB_groth16/generate_input.pyçš„åˆ†å—é€»è¾‘
        
        Args:
            buy_hash: ä¹°å®¶å“ˆå¸Œå€¼
            chunk_pixel_size: æ¯å—çš„åƒç´ æ•°é‡ (å³å‚æ•°m)
            output_session_dir: è¾“å‡ºä¼šè¯ç›®å½•ï¼ˆå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        
        Returns:
            åˆ†å—ç»“æœä¿¡æ¯
        """
        try:
            logging.info(f"å¼€å§‹åˆ†å—å¤„ç†ï¼Œbuy_hash: {buy_hash[:16]}..., chunk_size: {chunk_pixel_size}")
            
            # 1. æŸ¥æ‰¾å¯¹åº”çš„ZKè¾“å…¥æ–‡ä»¶
            zk_input_result = find_zk_input_for_buy_hash(buy_hash, self.data_dir)
            if not zk_input_result:
                raise ValueError(f"æœªæ‰¾åˆ°buy_hashå¯¹åº”çš„ZKè¾“å…¥æ–‡ä»¶: {buy_hash[:16]}...")
            
            zk_input_files = zk_input_result['zk_input_files']
            logging.info(f"æ‰¾åˆ° {len(zk_input_files)} ä¸ªZKè¾“å…¥æ–‡ä»¶")
            
            # 2. åˆ›å»ºè¾“å‡ºç›®å½•
            if output_session_dir is None:
                output_session_dir = os.path.join(
                    self.chunked_inputs_dir, 
                    f"session_{buy_hash[:16]}"
                )
            
            os.makedirs(output_session_dir, exist_ok=True)
            
            chunk_output_dir = os.path.join(output_session_dir, f"chunk_pixel_{chunk_pixel_size}")
            os.makedirs(chunk_output_dir, exist_ok=True)
            
            chunked_files = []
            processing_summary = []
            
            # 3. å¤„ç†æ¯ä¸ªZKè¾“å…¥æ–‡ä»¶ï¼ˆæ¯å¼ å›¾ç‰‡ï¼‰
            for img_idx, file_info in enumerate(zk_input_files):
                zk_file_path = file_info['file_path']
                filename = file_info['filename']
                
                logging.info(f"å¤„ç†ç¬¬ {img_idx + 1} å¼ å›¾ç‰‡: {filename}")
                
                # åŠ è½½ZKè¾“å…¥æ•°æ®
                with open(zk_file_path, 'r', encoding='utf-8') as f:
                    zk_data = json.load(f)
                
                metadata = zk_data['metadata']
                pixel_data = zk_data['pixel_data']
                
                # è·å–åƒç´ æ•°æ®ï¼ˆå·²ç»æ˜¯åˆ—ä¼˜å…ˆæ’åˆ—ï¼‰
                ori_pixels = np.array(pixel_data['original_pixels'])
                wm_pixels = np.array(pixel_data['watermarked_pixels'])
                binary_watermark = pixel_data['binary_watermark']
                
                total_pixels = metadata['total_pixels']
                image_dimensions = metadata['image_dimensions']
                
                logging.info(f"  å›¾ç‰‡å°ºå¯¸: {image_dimensions}, æ€»åƒç´ : {total_pixels}")
                
                # 4. æŒ‰ç…§LSB_groth16/generate_input.pyçš„é€»è¾‘è¿›è¡Œåˆ†å—
                chunked_files_for_image = self._chunk_single_image(
                    ori_pixels, wm_pixels, binary_watermark,
                    total_pixels, chunk_pixel_size,
                    chunk_output_dir, img_idx + 1
                )
                
                chunked_files.extend(chunked_files_for_image)
                
                processing_summary.append({
                    "image_index": img_idx + 1,
                    "original_file": filename,
                    "total_pixels": total_pixels,
                    "image_dimensions": image_dimensions,
                    "chunks_created": len(chunked_files_for_image)
                    # ç§»é™¤chunk_filesè¯¦ç»†åˆ—è¡¨ï¼Œå‡å°‘è¾“å‡ºå†—ä½™
                })
                
                logging.info(f"  âœ… å®Œæˆåˆ†å—ï¼Œç”Ÿæˆ {len(chunked_files_for_image)} ä¸ªåˆ†å—æ–‡ä»¶")
            
            # 5. ç”Ÿæˆä¼šè¯ä¿¡æ¯æ–‡ä»¶
            session_info = {
                "buy_hash": buy_hash,
                "chunk_pixel_size": chunk_pixel_size,
                "total_images": len(zk_input_files),
                "total_chunks": len(chunked_files),
                "output_directory": chunk_output_dir,
                "processing_summary": processing_summary,
                "chunked_files_count": len(chunked_files),  # åªä¿å­˜æ•°é‡ï¼Œä¸ä¿å­˜è¯¦ç»†åˆ—è¡¨
                "chunked_files_dir": chunk_output_dir,  # ä¿å­˜ç›®å½•è·¯å¾„
                "timestamp": time.time(),
                "format_version": "1.0"
            }
            
            session_info_file = os.path.join(output_session_dir, "session_info.json")
            with open(session_info_file, 'w', encoding='utf-8') as f:
                json.dump(session_info, f, indent=2, ensure_ascii=False)
            
            logging.info(f"âœ… åˆ†å—å¤„ç†å®Œæˆ:")
            logging.info(f"  - å¤„ç†å›¾ç‰‡æ•°: {len(zk_input_files)}")
            logging.info(f"  - ç”Ÿæˆåˆ†å—æ•°: {len(chunked_files)}")
            logging.info(f"  - è¾“å‡ºç›®å½•: {chunk_output_dir}")
            logging.info(f"  - ä¼šè¯ä¿¡æ¯: {session_info_file}")
            
            return {
                "success": True,
                "session_info": session_info,
                "session_info_file": session_info_file,
                "chunk_output_dir": chunk_output_dir,
                "total_chunks": len(chunked_files)
            }
        
        except Exception as e:
            logging.error(f"åˆ†å—å¤„ç†å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _chunk_single_image(self, ori_pixels: np.ndarray, wm_pixels: np.ndarray, 
                           binary_watermark: List[int], total_pixels: int, 
                           chunk_pixel_size: int, output_dir: str, image_index: int) -> List[str]:
        """
        å¯¹å•å¼ å›¾ç‰‡è¿›è¡Œåˆ†å—å¤„ç†
        å®Œå…¨æŒ‰ç…§LSB_groth16/generate_input.pyçš„é€»è¾‘
        
        Args:
            ori_pixels: åŸå§‹åƒç´ æ•°ç»„ (åˆ—ä¼˜å…ˆæ’åˆ—)
            wm_pixels: æ°´å°åƒç´ æ•°ç»„ (åˆ—ä¼˜å…ˆæ’åˆ—)
            binary_watermark: äºŒè¿›åˆ¶æ°´å°æ•°ç»„
            total_pixels: æ€»åƒç´ æ•°
            chunk_pixel_size: åˆ†å—å¤§å°
            output_dir: è¾“å‡ºç›®å½•
            image_index: å›¾ç‰‡ç´¢å¼•
        
        Returns:
            åˆ†å—æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        chunk_files = []
        
        # è®¡ç®—å¡«å……åƒç´ æ•°ï¼ˆå¦‚æœæœ€åä¸€å—ä¸å¤Ÿchunk_pixel_sizeï¼‰
        zero_pixels = chunk_pixel_size - (total_pixels % chunk_pixel_size) if (total_pixels % chunk_pixel_size != 0) else 0
        
        # è®¡ç®—æ‰©å±•åçš„æ°´å°å¤§å°ï¼Œå¹¶è¡¥å……è¶³å¤Ÿçš„0
        extended_watermark_size = total_pixels * 3 + zero_pixels * 3
        extended_watermark = binary_watermark + [0] * (extended_watermark_size - len(binary_watermark))
        
        # è®¡ç®—åˆ†å—æ•°
        chunks = (total_pixels + chunk_pixel_size - 1) // chunk_pixel_size
        
        logging.info(f"    åˆ†å—è®¡ç®—: æ€»åƒç´ ={total_pixels}, åˆ†å—å¤§å°={chunk_pixel_size}, åˆ†å—æ•°={chunks}, å¡«å……åƒç´ ={zero_pixels}")
        
        # ä¸ºå½“å‰å›¾ç‰‡åˆ›å»ºå­ç›®å½•
        image_output_dir = os.path.join(output_dir, f'input_{image_index}')
        os.makedirs(image_output_dir, exist_ok=True)
        
        for chunk_idx in range(chunks):
            start_idx = chunk_idx * chunk_pixel_size
            end_idx = min((chunk_idx + 1) * chunk_pixel_size, total_pixels)
            
            logging.debug(f"    å¤„ç†åˆ†å— {chunk_idx + 1}/{chunks}: åƒç´ èŒƒå›´ {start_idx}-{end_idx}")
            
            # åˆ†å—åï¼Œå¦‚æœæœ€åä¸€å—ä¸è¶³chunk_pixel_sizeï¼Œå°±éœ€è¦è¡¥é›¶
            if end_idx - start_idx < chunk_pixel_size:
                fill_size = chunk_pixel_size - (end_idx - start_idx)
                ori_fill = np.zeros((fill_size, 3), dtype=int)
                wm_fill = np.zeros((fill_size, 3), dtype=int)
                ori_block_pixels = np.vstack([ori_pixels[start_idx:end_idx], ori_fill])
                wm_block_pixels = np.vstack([wm_pixels[start_idx:end_idx], wm_fill])
            else:
                ori_block_pixels = ori_pixels[start_idx:end_idx]
                wm_block_pixels = wm_pixels[start_idx:end_idx]
            
            # ç‰¹åˆ«å¤„ç†æœ€åä¸€ä¸ªåˆ†å—çš„æ°´å°æ•°æ®
            if chunk_idx == chunks - 1:
                end_extend_idx = end_idx * 3 + zero_pixels * 3
                current_watermark = extended_watermark[start_idx * 3 : end_extend_idx]
            else:
                current_watermark = extended_watermark[start_idx * 3 : end_idx * 3]
            
            # ç”ŸæˆLSB_groth16æ ¼å¼çš„JSONæ•°æ®
            json_data = {
                "originalPixelValues": ori_block_pixels.tolist(),
                "Watermark_PixelValues": wm_block_pixels.tolist(),
                "binaryWatermark": current_watermark,
                "binaryWatermark_num": "513"
            }
            
            # ä¿å­˜åˆ†å—æ–‡ä»¶
            output_filename = os.path.join(image_output_dir, f'input_{image_index}_{chunk_idx + 1}.json')
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=4)
            
            chunk_files.append(output_filename)
            logging.debug(f"    âœ… ä¿å­˜åˆ†å—æ–‡ä»¶: {os.path.basename(output_filename)}")
        
        logging.info(f"    âœ… å›¾ç‰‡ {image_index} åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(chunk_files)} ä¸ªæ–‡ä»¶")
        return chunk_files

def process_watermarked_dataset_registration(buy_hash: str, optimal_config: Dict[str, Any], user_address: str = None) -> Dict[str, Any]:
    """
    åœ¨æ•°æ®é›†ç™»è®°æ—¶æ£€æµ‹åˆ°æ°´å°åçš„å¤„ç†æµç¨‹
    ä½¿ç”¨ç™»è®°æ—¶å·²è®¡ç®—çš„æœ€ä¼˜çº¦æŸå‚æ•°è¿›è¡Œåˆ†å—
    
    Args:
        buy_hash: æ£€æµ‹åˆ°çš„ä¹°å®¶å“ˆå¸Œå€¼
        optimal_config: ç™»è®°æ—¶è®¡ç®—çš„æœ€ä¼˜çº¦æŸé…ç½®ï¼ŒåŒ…å«Xã€Mã€må‚æ•°
        user_address: ç”¨æˆ·åœ°å€ï¼ˆç”¨äºç”Ÿæˆå”¯ä¸€çš„ç”¨æˆ·IDï¼‰
    
    Returns:
        å¤„ç†ç»“æœ
    """
    try:
        logging.info("=== ç¬¬ä¸‰é˜¶æ®µï¼šæ£€æµ‹åˆ°æ°´å°æ•°æ®é›†ï¼Œå¼€å§‹åˆ†å—å¤„ç† ===")
        logging.info(f"Buy hash: {buy_hash[:16]}...")
        logging.info(f"User address: {user_address}")
        logging.info(f"æœ€ä¼˜çº¦æŸé…ç½®: {optimal_config}")
        
        # æå–å…³é”®å‚æ•°
        chunk_pixel_size = optimal_config['m']  # æ¯å—åƒç´ æ•°
        constraint_power = optimal_config['power']  # X (å¹‚æ¬¡)
        num_chunks_expected = optimal_config['M']  # åˆ†å—æ•°
        
        logging.info(f"åˆ†å—å‚æ•°: m={chunk_pixel_size}, X=2^{constraint_power}, M={num_chunks_expected}")
        
        # åˆ›å»ºåˆ†å—å¤„ç†å™¨
        processor = Stage3ChunkProcessor()
        
        # æ‰§è¡Œåˆ†å—å¤„ç†
        result = processor.chunk_zk_input_data(
            buy_hash=buy_hash,
            chunk_pixel_size=chunk_pixel_size
        )
        
        if result['success']:
            # æ·»åŠ çº¦æŸé…ç½®ä¿¡æ¯åˆ°ç»“æœä¸­
            result['optimal_config'] = optimal_config
            result['ready_for_proof_generation'] = True
            
            logging.info("âœ… ç¬¬ä¸‰é˜¶æ®µåˆ†å—å¤„ç†å®Œæˆï¼")
            
            # ğŸš€ æ­£ç¡®çš„æµç¨‹ï¼šåˆ†å—å®Œæˆåï¼Œç«‹å³ç”ŸæˆPowers of Tauåˆå§‹æ–‡ä»¶
            # è€Œä¸æ˜¯ç«‹å³åˆ›å»ºLSBå®éªŒç›®å½•ï¼LSBå®éªŒç›®å½•åº”è¯¥ç­‰Powers of Tauå®Œæˆåå†åˆ›å»º
            
            # è·å–åˆ†å—ä¿¡æ¯
            chunk_pixel_size = optimal_config.get('m', 29)
            constraint_power = optimal_config.get('power', 16)
            total_chunks = result.get('total_chunks', 0)
            
            logging.info(f"âœ… ç¬¬ä¸‰é˜¶æ®µæ•°æ®åˆ†å—å¤„ç†å®Œæˆ")
            logging.info(f"åˆ†å—ç»“æœ: {total_chunks} ä¸ªåˆ†å—æ–‡ä»¶å·²ç”Ÿæˆ")
            logging.info(f"å‡†å¤‡Powers of Tauåˆå§‹æ–‡ä»¶ï¼Œç­‰å¾…ç”¨æˆ·åœ¨æµè§ˆå™¨ä¸­è´¡çŒ®")
            
            # ç”ŸæˆPowers of Tauåˆå§‹æ–‡ä»¶ï¼Œå‡†å¤‡ç”¨æˆ·è´¡çŒ®
            try:
                from features.poweroftau_generator import PowerOfTauGenerator
                generator = PowerOfTauGenerator()
                
                # ğŸ”§ Bugä¿®å¤ï¼šä½¿ç”¨ç”¨æˆ·åœ°å€ç”Ÿæˆå”¯ä¸€çš„ç”¨æˆ·IDï¼Œè€Œä¸æ˜¯ä¹°å®¶å“ˆå¸Œ
                # è¿™æ ·æ¯ä¸ªä¸åŒçš„ç”¨æˆ·éƒ½ä¼šæœ‰ä¸åŒçš„ç”¨æˆ·ID
                if user_address:
                    user_id = user_address.replace('0x', '')[:8].upper()
                    user_id_source = "ç”¨æˆ·åœ°å€"
                else:
                    # å¦‚æœæ²¡æœ‰ç”¨æˆ·åœ°å€ï¼Œå›é€€åˆ°ä¹°å®¶å“ˆå¸Œï¼ˆå…¼å®¹æ€§ï¼‰
                    user_id = buy_hash[:8].upper()
                    user_id_source = "ä¹°å®¶å“ˆå¸Œ"
                    
                logging.info(f"ç”Ÿæˆçš„ç”¨æˆ·ID: {user_id} (æ¥æº: {user_id_source})")
                
                # ç”ŸæˆPowers of Tauåˆå§‹æ–‡ä»¶
                ptau_result = generator.generate_initial_ptau_for_user_contribution(
                    constraint_power=constraint_power,
                    user_id=user_id
                )
                
                if ptau_result.get("status") == "success":
                    logging.info(f"âœ… Powers of Tauåˆå§‹æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {ptau_result.get('initial_ptau_path')}")
                    
                    # åˆ›å»ºåˆ†å—å®ŒæˆçŠ¶æ€æ–‡ä»¶ï¼Œä¾›Powers of Tauå®Œæˆåä½¿ç”¨
                    current_dir = os.path.dirname(os.path.abspath(__file__))
                    project_root = os.path.dirname(os.path.dirname(current_dir))
                    lsb_dir = os.path.join(project_root, "LSB_groth16")
                    
                    stage3_status_file = os.path.join(lsb_dir, f"stage3_completed_{user_id}.json")
                    stage3_status = {
                        "buyer_hash": buy_hash,
                        "buyer_hash_16": buy_hash[:16],
                        "chunk_pixel_size": chunk_pixel_size,
                        "constraint_power": constraint_power,
                        "total_chunks": total_chunks,
                        "optimal_config": optimal_config,
                        "chunked_data_dir": result.get('chunk_output_dir'),
                        "stage3_completion_time": time.time(),
                        "user_id": user_id,
                        "user_address": user_address,  # ğŸ”§ ä¿å­˜ç”¨æˆ·åœ°å€ä¾›åç»­ä½¿ç”¨
                        "user_id_source": user_id_source,  # è®°å½•ç”¨æˆ·IDæ¥æº
                        "status": "stage3_completed_waiting_ptau",
                        "ready_for_stage4": False  # ç­‰Powers of Tauå®Œæˆåè®¾ä¸ºTrue
                    }
                    
                    with open(stage3_status_file, 'w') as f:
                        json.dump(stage3_status, f, indent=2)
                    
                    logging.info(f"âœ… é˜¶æ®µ3çŠ¶æ€æ–‡ä»¶å·²ä¿å­˜: {stage3_status_file}")
                    
                    return {
                        "status": "copyright_violation",
                        "message": "æ£€æµ‹åˆ°ä¾µæƒè¡Œä¸ºï¼Œéœ€è¦æ‚¨è´¡çŒ®éšæœºæ€§ä»¥ç”Ÿæˆé›¶çŸ¥è¯†è¯æ˜",
                        "background_message": "ç³»ç»Ÿæ£€æµ‹åˆ°è¯¥æ•°æ®é›†åŒ…å«æ°´å°ï¼Œè¡¨æ˜å¯èƒ½æ˜¯ä»å…¶ä»–åœ°æ–¹è´­ä¹°åè½¬å”®ã€‚ä¸ºäº†ç”Ÿæˆé›¶çŸ¥è¯†è¯æ˜ï¼Œéœ€è¦æ‚¨åœ¨æµè§ˆå™¨ä¸­è´¡çŒ®ä¸€äº›éšæœºæ€§ã€‚",
                        "requires_user_contribution": True,
                        "user_id": user_id,
                        "constraint_power": constraint_power,
                        "ptau_info": {
                            "initial_ptau_path": ptau_result.get("initial_ptau_path"),
                            "constraint_power": constraint_power,
                            "total_chunks": total_chunks,
                            "chunk_pixel_size": chunk_pixel_size
                        },
                        "zk_proof_result": {
                            "stage": "stage3_chunking_completed",
                            "chunks_generated": total_chunks,
                            "next_step": "powers_of_tau_contribution"
                        }
                    }
                else:
                    logging.error(f"âŒ Powers of Tauåˆå§‹æ–‡ä»¶ç”Ÿæˆå¤±è´¥: {ptau_result.get('error')}")
                    return {
                        "status": "stage3_failed",
                        "message": f"Powers of Tauåˆå§‹åŒ–å¤±è´¥: {ptau_result.get('error')}",
                        "error": ptau_result.get('error')
                    }
                    
            except Exception as ptau_e:
                logging.error(f"âŒ Powers of Tauç”Ÿæˆå¤±è´¥: {ptau_e}")
                return {
                    "status": "stage3_failed", 
                    "message": f"Powers of Tauç”Ÿæˆå¤±è´¥: {str(ptau_e)}",
                    "error": str(ptau_e)
                }
        else:
            logging.error(f"åˆ†å—å¤„ç†å¤±è´¥: {result['error']}")
            return {
                "status": "chunking_failed",
                "message": f"åˆ†å—å¤„ç†å¤±è´¥: {result['error']}",
                "error": result['error']
            }
    
    except Exception as e:
        logging.error(f"ç¬¬ä¸‰é˜¶æ®µå¤„ç†å¤±è´¥: {str(e)}")
        return {
            "status": "stage3_failed", 
            "message": f"ç¬¬ä¸‰é˜¶æ®µå¤„ç†å¤±è´¥: {str(e)}",
            "error": str(e)
        }

# ä¸ºäº†å…¼å®¹æ€§ï¼Œå¯¼å…¥timeæ¨¡å—
import time

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    # ç¤ºä¾‹å‚æ•°ï¼ˆå®é™…ä½¿ç”¨æ—¶ä»ç™»è®°é˜¶æ®µè·å–ï¼‰
    test_buy_hash = "4b9f3139de796443209eefd90bf43deff2d5bd4902346e604022bcd825f28136e"
    test_optimal_config = {
        "power": 19,
        "constraint_size": 524288, 
        "M": 4,
        "m": 7140,
        "total_time": 120.0
    }
    
    print("=== ç¬¬ä¸‰é˜¶æ®µæµ‹è¯• ===")
    result = process_watermarked_dataset_registration(test_buy_hash, test_optimal_config)
    print(json.dumps(result, indent=2, ensure_ascii=False)) 