import os
import shutil
import zipfile
import logging
import json
import hashlib
import time
from PIL import Image
import numpy as np
from web3 import Web3

def add_lsb_watermark(image_path, buy_hash, output_path):
    """
    å°†buy_hashæ°´å°ä¿¡æ¯åµŒå…¥åˆ°å›¾åƒçš„æœ€ä½æœ‰æ•ˆä½(LSB)
    ä½¿ç”¨ä¸LSB_groth16å®Œå…¨ä¸€è‡´çš„åˆ—ä¼˜å…ˆåµŒå…¥æ–¹å¼
    
    å‚æ•°:
    image_path: åŸå§‹å›¾åƒè·¯å¾„
    buy_hash: è¦åµŒå…¥çš„buy_hashå€¼ (å­—ç¬¦ä¸²)
    output_path: è¾“å‡ºå›¾åƒè·¯å¾„
    
    è¿”å›:
    bool: æˆåŠŸè¿”å›Trueï¼Œå¤±è´¥è¿”å›False
    """
    try:
        # åŠ è½½å›¾åƒå¹¶è½¬ä¸ºRGB (ä¸LSB_groth16ä¸€è‡´)
        image = Image.open(image_path).convert('RGB')
        pixel = image.load()
        width, height = image.size
        
        # å°†buy_hashè½¬æ¢ä¸ºäºŒè¿›åˆ¶å­—ç¬¦ä¸² (ä¿æŒåŸæœ‰é€»è¾‘)
        binary_watermark = ''.join([bin(ord(c))[2:].rjust(8, '0') for c in buy_hash])
        
        # è®¡ç®—æ€»å®¹é‡å¹¶å¡«å…… (å…³é”®ï¼šä¸LSB_groth16ä¸€è‡´)
        total_capacity = width * height * 3
        if len(binary_watermark) < total_capacity:
            padded_secret = binary_watermark + '0' * (total_capacity - len(binary_watermark))
        else:
            padded_secret = binary_watermark[:total_capacity]
        
        # åˆ—ä¼˜å…ˆéå†åµŒå…¥ (å…³é”®ï¼šä¸LSB_groth16å®Œå…¨ä¸€è‡´)
        index = 0
        for x in range(width):  # åˆ—ä¼˜å…ˆ
            for y in range(height):
                r, g, b = pixel[x, y]
                # RGBä¸‰é€šé“LSBåµŒå…¥
                r = (r & 0xFE) | int(padded_secret[index])
                index += 1
                g = (g & 0xFE) | int(padded_secret[index])
                index += 1
                b = (b & 0xFE) | int(padded_secret[index])
                index += 1
                pixel[x, y] = (r, g, b)
        
        # ä¿å­˜å›¾åƒ
        image.save(output_path, format='PNG')
        return True
    
    except Exception as e:
        logging.error(f"ä¸ºå›¾åƒ {os.path.basename(image_path)} æ·»åŠ æ°´å°æ—¶å‡ºé”™: {str(e)}")
        return False

def generate_watermark_from_contract(token_id, buyer_address, sale_hash=None):
    """
    åŸºäºåˆçº¦æ•°æ®ç”Ÿæˆæ°´å°ä¿¡æ¯
    
    å‚æ•°:
    token_id: NFTçš„tokenId
    buyer_address: ä¹°å®¶çš„ä»¥å¤ªåŠåœ°å€
    sale_hash: å¦‚æœå·²çŸ¥ï¼Œç›´æ¥ä½¿ç”¨è¯¥SaleHashå€¼ï¼Œå¦åˆ™ä»åˆçº¦è·å–
    
    è¿”å›:
    tuple: (æ°´å°JSONå­—ç¬¦ä¸², buy_hashå­—ç¬¦ä¸²)
    """
    timestamp = int(time.time())
    
    # å¦‚æœæœªæä¾›sale_hashï¼Œå°è¯•ä»åˆçº¦è·å–
    if not sale_hash:
        try:
            # åˆå§‹åŒ–Web3è¿æ¥
            w3 = Web3(Web3.HTTPProvider("http://127.0.0.1:8545"))
            
            # è¯»å–åˆçº¦ä¿¡æ¯
            deploy_info_path = os.path.join(os.path.dirname(__file__), "..", "..", "python_call_contract", "deploy_address.json")
            with open(deploy_info_path, "r") as f:
                deploy_info = json.load(f)
            
            data_registration_addr = deploy_info["DataRegistration"]
            
            # åŠ è½½ABI
            abi_path = os.path.join(os.path.dirname(__file__), "..", "..", "BDTP_contract", "out", "DataRegistration.sol", "DataRegistration.json")
            with open(abi_path, "r") as f:
                artifact = json.load(f)
                data_registration_abi = artifact["abi"]
            
            # åˆå§‹åŒ–åˆçº¦
            contract = w3.eth.contract(address=data_registration_addr, abi=data_registration_abi)
            
            # è°ƒç”¨getTokenIdToSaleHashè·å–SaleHash
            sale_hash = contract.functions.getTokenIdToSaleHash(token_id).call()
            
            # å°†bytes32è½¬æ¢ä¸º16è¿›åˆ¶å­—ç¬¦ä¸²
            sale_hash = sale_hash.hex()
            logging.info(f"ä»åˆçº¦è·å–SaleHash: {sale_hash}")
        except Exception as e:
            logging.error(f"ä»åˆçº¦è·å–SaleHashå¤±è´¥: {str(e)}")
            # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨ä¸€ä¸ªåŸºäºtokenIdçš„æ¨¡æ‹Ÿå€¼
            sale_hash = hashlib.sha256(f"fallback_sale_hash_{token_id}".encode()).hexdigest()
            logging.warning(f"ä½¿ç”¨åå¤‡SaleHash: {sale_hash}")
    
    # è®¡ç®—buyHash = H(SaleHash, Buyer_address, timestamp)
    buy_hash_input = f"{sale_hash}{buyer_address.lower()}{timestamp}"
    buy_hash = hashlib.sha256(buy_hash_input.encode()).hexdigest()
    
    # æ„å»ºæ°´å°ä¿¡æ¯JSONå¯¹è±¡
    watermark_data = {
        "token_id": token_id,
        "buyer_address": buyer_address,
        "sale_hash": sale_hash,
        "timestamp": timestamp,
        "buy_hash": buy_hash
    }
    
    # æ·»åŠ æ ¡éªŒå’Œ
    data_str = json.dumps(watermark_data, sort_keys=True)
    checksum = hashlib.sha256(data_str.encode()).hexdigest()[:8]
    watermark_data["checksum"] = checksum
    
    return json.dumps(watermark_data), buy_hash

def process_image_folder(input_folder, output_folder, buy_hash):
    """
    å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒï¼Œæ·»åŠ æ°´å°
    
    å‚æ•°:
    input_folder: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
    output_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    buy_hash: è¦åµŒå…¥çš„buy_hashå€¼
    
    è¿”å›:
    int: æˆåŠŸå¤„ç†çš„å›¾åƒæ•°é‡
    """
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    processed_count = 0
    supported_formats = ['.png', '.jpg', '.jpeg', '.bmp', '.gif']
    
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if any(file.lower().endswith(fmt) for fmt in supported_formats):
                input_path = os.path.join(root, file)
                # è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œä»¥ä¾¿åœ¨è¾“å‡ºç›®å½•ä¸­åˆ›å»ºç›¸åŒçš„ç»“æ„
                rel_path = os.path.relpath(input_path, input_folder)
                output_path = os.path.join(output_folder, rel_path)
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                # æ·»åŠ æ°´å°
                if add_lsb_watermark(input_path, buy_hash, output_path):
                    processed_count += 1
                    logging.info(f"æˆåŠŸä¸º {rel_path} æ·»åŠ æ°´å°")
            else:
                # å¯¹äºéå›¾åƒæ–‡ä»¶ï¼Œç›´æ¥å¤åˆ¶
                input_path = os.path.join(root, file)
                rel_path = os.path.relpath(input_path, input_folder)
                output_path = os.path.join(output_folder, rel_path)
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                shutil.copy2(input_path, output_path)
    
    return processed_count

def generate_zk_input_data(original_image_path, watermarked_image_path, buy_hash, output_dir, image_filename=None):
    """
    åœ¨è´­ä¹°æ—¶ç”ŸæˆZKè¯æ˜æ‰€éœ€çš„å®Œæ•´è¾“å…¥æ•°æ®ï¼ˆç¬¬äºŒé˜¶æ®µï¼‰
    ä½¿ç”¨åˆ—ä¼˜å…ˆæ–¹å¼è¯»å–åƒç´ å€¼ï¼Œä¸LSB_groth16ä¸€è‡´
    
    å‚æ•°:
    original_image_path: åŸå§‹å›¾åƒè·¯å¾„
    watermarked_image_path: æ°´å°å›¾åƒè·¯å¾„  
    buy_hash: ä¹°å®¶å“ˆå¸Œå€¼
    output_dir: è¾“å‡ºç›®å½•
    image_filename: å›¾åƒæ–‡ä»¶åï¼ˆç”¨äºåŒºåˆ†ä¸åŒå›¾ç‰‡ï¼‰
    
    è¿”å›:
    str: ç”Ÿæˆçš„è¾“å…¥æ–‡ä»¶è·¯å¾„
    """
    try:
        logging.info(f"å¼€å§‹ç”ŸæˆZKè¾“å…¥æ•°æ®ï¼Œbuy_hash: {buy_hash[:16]}...")
        
        # 1. åŠ è½½åŸå§‹å’Œæ°´å°å›¾åƒ
        ori_img = Image.open(original_image_path).convert('RGB')
        wm_img = Image.open(watermarked_image_path).convert('RGB')
        
        width, height = ori_img.size
        total_pixels = width * height
        
        logging.info(f"å›¾åƒå°ºå¯¸: {width}x{height}, æ€»åƒç´ æ•°: {total_pixels}")
        
        # 2. å…³é”®ï¼šä½¿ç”¨åˆ—ä¼˜å…ˆæ–¹å¼å±•å¹³åƒç´  (ä¸LSB_groth16ä¸€è‡´)
        # å°†PILå›¾åƒè½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œç„¶åæŒ‰åˆ—ä¼˜å…ˆé‡æ’
        ori_array = np.array(ori_img)
        wm_array = np.array(wm_img)
        
        # åˆ—ä¼˜å…ˆå±•å¹³ï¼šæŒ‰(x,y)é¡ºåºï¼Œå³å…ˆéå†xè½´ï¼ˆåˆ—ï¼‰ï¼Œå†éå†yè½´ï¼ˆè¡Œï¼‰
        ori_pixels = []
        wm_pixels = []
        
        for x in range(width):  # åˆ—ä¼˜å…ˆ
            for y in range(height):
                ori_pixels.append(ori_array[y, x].tolist())  # numpyæ˜¯[y,x]ç´¢å¼•
                wm_pixels.append(wm_array[y, x].tolist())
        
        # 3. å°†buy_hashè½¬æ¢ä¸ºäºŒè¿›åˆ¶æ•°ç»„
        binary_watermark = []
        for c in buy_hash:
            binary_watermark.extend([int(b) for b in bin(ord(c))[2:].rjust(8, '0')])
        
        # 4. æ‰©å±•æ°´å°åˆ°å…¨å®¹é‡ (ä¸LSB_groth16ä¸€è‡´)
        extended_watermark_size = total_pixels * 3
        if len(binary_watermark) < extended_watermark_size:
            extended_watermark = binary_watermark + [0] * (extended_watermark_size - len(binary_watermark))
        else:
            extended_watermark = binary_watermark[:extended_watermark_size]
        
        # 5. ç”Ÿæˆå®Œæ•´è¾“å…¥æ•°æ®
        zk_input_data = {
            "metadata": {
                "buy_hash": buy_hash,
                "total_pixels": total_pixels,
                "image_dimensions": [width, height],
                "watermark_length": len(binary_watermark),
                "timestamp": time.time(),
                "format_version": "1.0",
                "traversal_order": "column_major"
            },
            "pixel_data": {
                "original_pixels": ori_pixels,
                "watermarked_pixels": wm_pixels,
                "binary_watermark": extended_watermark
            },
            "verification": {
                "total_capacity": extended_watermark_size,
                "used_capacity": len(binary_watermark),
                "padding_zeros": extended_watermark_size - len(binary_watermark)
            }
        }
        
        # 6. ä¿å­˜å®Œæ•´è¾“å…¥æ•°æ®
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆç‹¬ç«‹çš„æ–‡ä»¶å
        if image_filename:
            # å»æ‰æ‰©å±•å
            base_name = os.path.splitext(image_filename)[0]
            output_file = os.path.join(output_dir, f"zk_input_{buy_hash[:16]}_{base_name}.json")
        else:
            output_file = os.path.join(output_dir, f"complete_zk_input_{buy_hash[:16]}.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(zk_input_data, f, indent=2, ensure_ascii=False)
        
        logging.info(f"ZKè¾“å…¥æ•°æ®å·²ä¿å­˜: {output_file}")
        logging.info(f"æ–‡ä»¶å¤§å°: {os.path.getsize(output_file) / 1024:.1f} KB")
        
        return output_file
    
    except Exception as e:
        logging.error(f"ç”ŸæˆZKè¾“å…¥æ•°æ®å¤±è´¥: {str(e)}")
        return None

def find_zk_input_for_buy_hash(buy_hash, data_dir=None):
    """
    æ ¹æ®buy_hashæŸ¥æ‰¾å¯¹åº”çš„ZKè¾“å…¥æ•°æ®æ–‡ä»¶
    
    å‚æ•°:
    buy_hash: ä¹°å®¶å“ˆå¸Œå€¼
    data_dir: æ•°æ®ç›®å½•ï¼Œé»˜è®¤ä¸ºç›¸å¯¹è·¯å¾„çš„dataç›®å½•
    
    è¿”å›:
    dict: åŒ…å«ZKè¾“å…¥æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
    """
    if data_dir is None:
        data_dir = os.path.join(os.path.dirname(__file__), "..", "data")
    
    # æŸ¥æ‰¾ZKè¾“å…¥ç›®å½•
    zk_input_dir = os.path.join(data_dir, "zk_inputs")
    if not os.path.exists(zk_input_dir):
        logging.warning(f"ZKè¾“å…¥ç›®å½•ä¸å­˜åœ¨: {zk_input_dir}")
        return None
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶ï¼ˆæ”¯æŒæ–°çš„å‘½åæ ¼å¼ï¼‰
    matching_files = []
    buy_hash_prefix = buy_hash[:16]
    
    for file in os.listdir(zk_input_dir):
        if file.startswith(f"zk_input_{buy_hash_prefix}_") and file.endswith(".json"):
            full_path = os.path.join(zk_input_dir, file)
            matching_files.append({
                "file_path": full_path,
                "filename": file,
                "file_size": os.path.getsize(full_path),
                "modified_time": os.path.getmtime(full_path)
            })
    
    # ä¹Ÿæ£€æŸ¥æ—§æ ¼å¼çš„æ–‡ä»¶
    old_pattern = f"complete_zk_input_{buy_hash_prefix}.json"
    old_file = os.path.join(zk_input_dir, old_pattern)
    if os.path.exists(old_file):
        matching_files.append({
            "file_path": old_file,
            "filename": old_pattern,
            "file_size": os.path.getsize(old_file),
            "modified_time": os.path.getmtime(old_file)
        })
    
    if matching_files:
        logging.info(f"æ‰¾åˆ° {len(matching_files)} ä¸ªZKè¾“å…¥æ–‡ä»¶ï¼Œbuy_hash: {buy_hash_prefix}...")
        return {
            "buy_hash": buy_hash,
            "zk_input_files": matching_files,
            "total_files": len(matching_files)
        }
    else:
        logging.warning(f"æœªæ‰¾åˆ°å¯¹åº”çš„ZKè¾“å…¥æ–‡ä»¶ï¼Œbuy_hash: {buy_hash_prefix}...")
        return None

def main(token_id=None, buyer_address=None, sale_hash=None):
    """
    ä¸»å‡½æ•°ï¼Œå¤„ç†æ•´ä¸ªæ°´å°æµç¨‹
    ç¬¬äºŒé˜¶æ®µï¼šå¢åŠ ZKè¾“å…¥æ•°æ®ç”Ÿæˆ
    
    å‚æ•°:
    token_id: NFTçš„tokenId (å¯é€‰)
    buyer_address: ä¹°å®¶çš„ä»¥å¤ªåŠåœ°å€ (å¯é€‰)
    sale_hash: ä»åˆçº¦è·å–çš„SaleHash (å¯é€‰)
    """
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    dataset_zip_path = os.path.join(DATA_DIR, "dataset.zip")              # data/dataset.zip
    dataset_folder = os.path.join(DATA_DIR, "dataset")                    # data/dataset/
    watermark_folder = os.path.join(DATA_DIR, "dataset_watermark")        # data/dataset_watermark/
    watermark_zip_path = os.path.join(DATA_DIR, "dataset_watermark.zip")  # data/dataset_watermark.zip
    watermark_info_path = os.path.join(DATA_DIR, "watermark_info.json")   # ä¿å­˜æ°´å°ä¿¡æ¯
    zk_input_dir = os.path.join(DATA_DIR, "zk_inputs")                    # ZKè¾“å…¥æ•°æ®ç›®å½•
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.isfile(dataset_zip_path):
        logging.error(f"{dataset_zip_path} æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰§è¡Œæ°´å°æµç¨‹ã€‚")
        return False
    
    # æ¸…ç†æ—§æ–‡ä»¶
    if os.path.exists(dataset_folder):
        shutil.rmtree(dataset_folder)
    if os.path.exists(watermark_folder):
        shutil.rmtree(watermark_folder)
    if os.path.isfile(watermark_zip_path):
        os.remove(watermark_zip_path)
    
    # 1) è§£å‹ dataset.zip åˆ° dataset/
    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:
        zip_ref.extractall(dataset_folder)
    logging.info(f"å·²è§£å‹ {dataset_zip_path} åˆ° {dataset_folder}")
    
    # ============ å¤„ç†macOSç”Ÿæˆçš„ç‰¹æ®Šæ–‡ä»¶å¤¹ ============
    # åˆ é™¤ __MACOSX æ–‡ä»¶å¤¹
    macosx_dir = os.path.join(dataset_folder, "__MACOSX")
    if os.path.exists(macosx_dir):
        shutil.rmtree(macosx_dir)
        logging.info(f"å·²åˆ é™¤: {macosx_dir}")
    
    # æ£€æŸ¥å¹¶å¤„ç†å­æ–‡ä»¶å¤¹
    subdirs = [d for d in os.listdir(dataset_folder) 
               if os.path.isdir(os.path.join(dataset_folder, d)) and not d.startswith('__MACOSX')]
    
    for subdir in subdirs:
        subdir_path = os.path.join(dataset_folder, subdir)
        for filename in os.listdir(subdir_path):
            src = os.path.join(subdir_path, filename)
            dst = os.path.join(dataset_folder, filename)
            shutil.move(src, dst)
        shutil.rmtree(subdir_path)
        logging.info(f"å·²ç§»åŠ¨å¹¶åˆ é™¤å­æ–‡ä»¶å¤¹: {subdir_path}")
    
    # 2) ç”Ÿæˆæ°´å°ä¿¡æ¯
    watermark_info = None
    if token_id and buyer_address:
        # å¦‚æœæä¾›äº†token_idå’Œbuyer_addressï¼Œä½¿ç”¨åˆçº¦ä¿¡æ¯ç”Ÿæˆæ°´å°
        watermark_info, buy_hash = generate_watermark_from_contract(token_id, buyer_address, sale_hash)
        logging.info(f"å·²ç”ŸæˆåŸºäºåˆçº¦çš„æ°´å°ä¿¡æ¯ï¼Œä½¿ç”¨buy_hash: {buy_hash}")
    else:
        # å¦åˆ™ç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„æ°´å°ä¿¡æ¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        timestamp = int(time.time())
        buy_hash = hashlib.sha256(f"test_{timestamp}".encode()).hexdigest()
        logging.info(f"å·²ç”Ÿæˆæµ‹è¯•buy_hash: {buy_hash}")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•æ°´å°ä¿¡æ¯
        test_watermark = {
            "timestamp": timestamp,
            "note": f"This is a test watermark created at {timestamp}",
            "buy_hash": buy_hash,
            "checksum": hashlib.sha256(f"test_{timestamp}".encode()).hexdigest()[:8]
        }
        watermark_info = json.dumps(test_watermark)
    
    # 3) å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰å›¾åƒï¼Œæ·»åŠ æ°´å°
    processed_count = process_image_folder(dataset_folder, watermark_folder, buy_hash)
    logging.info(f"å·²ä¸º {processed_count} ä¸ªæ–‡ä»¶æ·»åŠ æ°´å°")
    
    # ============ ç¬¬äºŒé˜¶æ®µï¼šç”ŸæˆZKè¾“å…¥æ•°æ® ============
    logging.info("å¼€å§‹ç¬¬äºŒé˜¶æ®µï¼šä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆé›¶çŸ¥è¯†è¯æ˜è¾“å…¥æ•°æ®")
    
    supported_formats = ['.png', '.jpg', '.jpeg', '.bmp', '.gif']
    zk_input_files = []
    
    # ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆç‹¬ç«‹çš„ZKè¾“å…¥æ•°æ®
    image_files = [f for f in os.listdir(dataset_folder) 
                   if any(f.lower().endswith(fmt) for fmt in supported_formats)]
    
    for file in image_files:
        original_image = os.path.join(dataset_folder, file)
        watermarked_image = os.path.join(watermark_folder, file)
        
        if os.path.exists(watermarked_image):
            # ä¸ºæ¯å¼ å›¾ç‰‡ç”Ÿæˆç‹¬ç«‹çš„ZKè¾“å…¥æ•°æ®
            zk_input_file = generate_zk_input_data(
                original_image, 
                watermarked_image, 
                buy_hash, 
                zk_input_dir,
                image_filename=file  # ä¼ é€’æ–‡ä»¶åç”¨äºåŒºåˆ†
            )
            
            if zk_input_file:
                zk_input_files.append(zk_input_file)
                logging.info(f"âœ… ZKè¾“å…¥æ•°æ®ç”ŸæˆæˆåŠŸ: {os.path.basename(zk_input_file)}")
            else:
                logging.warning(f"âš ï¸  å›¾ç‰‡ {file} çš„ZKè¾“å…¥æ•°æ®ç”Ÿæˆå¤±è´¥")
        else:
            logging.warning(f"âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡ {file} çš„æ°´å°ç‰ˆæœ¬")
    
    if zk_input_files:
        logging.info(f"âœ… æ€»å…±ä¸º {len(zk_input_files)} å¼ å›¾ç‰‡ç”Ÿæˆäº†ZKè¾“å…¥æ•°æ®")
    else:
        logging.warning("âš ï¸  æœªèƒ½ç”Ÿæˆä»»ä½•ZKè¾“å…¥æ•°æ®")
    
    # 4) ä¿å­˜æ°´å°ä¿¡æ¯åˆ°æ–‡ä»¶ï¼Œä¾›ä»¥åæ£€æµ‹ä½¿ç”¨ - ä¿®æ”¹ä¸ºè¿½åŠ æ¨¡å¼
    watermark_record = {
        "buy_hash": buy_hash,
        "watermark_info": watermark_info,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "token_id": token_id,
        "buyer_address": buyer_address,
        "processed_files": processed_count,
        "zk_input_files": zk_input_files if 'zk_input_files' in locals() and zk_input_files else []  # æ·»åŠ ZKè¾“å…¥æ–‡ä»¶åˆ—è¡¨
    }
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™åŠ è½½ç°æœ‰æ•°æ®
    if os.path.exists(watermark_info_path):
        try:
            with open(watermark_info_path, 'r') as f:
                existing_data = json.load(f)
                
            # å¦‚æœç°æœ‰æ•°æ®æ˜¯æ—§æ ¼å¼ï¼ˆå•ä¸ªè®°å½•ï¼‰ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼ï¼ˆå¤šä¸ªè®°å½•ï¼‰
            if "buy_hash" in existing_data and "records" not in existing_data:
                # æ—§æ ¼å¼ï¼Œè½¬æ¢ä¸ºæ–°æ ¼å¼
                new_data = {
                    "records": [existing_data],
                    "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "total_records": 1
                }
                existing_data = new_data
            elif "records" not in existing_data:
                # å¦‚æœæ²¡æœ‰recordså­—æ®µï¼Œåˆ›å»ºæ–°çš„ç»“æ„
                existing_data = {
                    "records": [],
                    "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "total_records": 0
                }
        except (json.JSONDecodeError, FileNotFoundError):
            # æ–‡ä»¶æŸåæˆ–ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„æ•°æ®ç»“æ„
            existing_data = {
                "records": [],
                "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
                "total_records": 0
            }
    else:
        # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„æ•°æ®ç»“æ„
        existing_data = {
            "records": [],
            "last_updated": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_records": 0
        }
    
    # æ·»åŠ æ–°çš„æ°´å°è®°å½•
    existing_data["records"].append(watermark_record)
    existing_data["last_updated"] = time.strftime("%Y-%m-%d %H:%M:%S")
    existing_data["total_records"] = len(existing_data["records"])
    
    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    with open(watermark_info_path, 'w') as f:
        json.dump(existing_data, f, indent=4)
    
    logging.info(f"å·²ä¿å­˜æ°´å°ä¿¡æ¯åˆ°: {watermark_info_path}ï¼Œå½“å‰å…±æœ‰ {existing_data['total_records']} æ¡è®°å½•")
    
    # 5) å‹ç¼© dataset_watermark => dataset_watermark.zip
    with zipfile.ZipFile(watermark_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, dirs, files in os.walk(watermark_folder):
            for file in files:
                fullpath = os.path.join(root, file)
                arcname = os.path.relpath(fullpath, watermark_folder)
                zipf.write(fullpath, arcname)
    
    logging.info(f"å·²ç”Ÿæˆæ°´å°å‹ç¼©åŒ…: {watermark_zip_path}")
    
    # ============ ç¬¬äºŒé˜¶æ®µå®Œæˆæ€»ç»“ ============
    logging.info("ğŸ‰ æ°´å°åµŒå…¥å’ŒZKè¾“å…¥æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    logging.info("âœ… ç¬¬ä¸€é˜¶æ®µï¼šç»Ÿä¸€æ°´å°å¤„ç† - å®Œæˆ")
    logging.info("âœ… ç¬¬äºŒé˜¶æ®µï¼šZKè¾“å…¥æ•°æ®ç”Ÿæˆ - å®Œæˆ")
    logging.info("ğŸ“‹ åç»­æ­¥éª¤ï¼šåœ¨æ•°æ®é›†ç™»è®°æ—¶è¿›è¡Œåˆ†å—å’Œè¯æ˜ç”Ÿæˆ")
    
    return True

if __name__ == "__main__":
    import sys
    
    # ä»å‘½ä»¤è¡Œè·å–å‚æ•°
    token_id = None
    buyer_address = None
    sale_hash = None
    
    if len(sys.argv) > 1:
        token_id = int(sys.argv[1])
    if len(sys.argv) > 2:
        buyer_address = sys.argv[2]
    if len(sys.argv) > 3:
        sale_hash = sys.argv[3]
    
    main(token_id, buyer_address, sale_hash)