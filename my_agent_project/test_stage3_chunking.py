#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç¬¬ä¸‰é˜¶æ®µåˆ†å—æµ‹è¯•è„šæœ¬
éªŒè¯æ•°æ®åˆ†å—åŠŸèƒ½æ˜¯å¦æŒ‰ç…§LSB_groth16/generate_input.pyçš„é€»è¾‘æ­£ç¡®å·¥ä½œ
"""

import os
import sys
import json
import logging

# æ·»åŠ featuresç›®å½•åˆ°path
sys.path.append(os.path.join(os.path.dirname(__file__), 'features'))

from stage3_chunk_and_proof import process_watermarked_dataset_registration, Stage3ChunkProcessor
from addWatermark import find_zk_input_for_buy_hash

def test_stage3_chunking():
    """æµ‹è¯•ç¬¬ä¸‰é˜¶æ®µåˆ†å—åŠŸèƒ½"""
    print("=== ç¬¬ä¸‰é˜¶æ®µåˆ†å—æµ‹è¯• ===")
    
    # 1. ä»æ•°æ®ç›®å½•æŸ¥æ‰¾ç°æœ‰çš„ZKè¾“å…¥æ–‡ä»¶
    data_dir = os.path.join(os.path.dirname(__file__), "data")
    zk_inputs_dir = os.path.join(data_dir, "zk_inputs")
    
    if not os.path.exists(zk_inputs_dir):
        print("âŒ ZKè¾“å…¥ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # è·å–ZKè¾“å…¥æ–‡ä»¶åˆ—è¡¨
    zk_files = [f for f in os.listdir(zk_inputs_dir) if f.startswith("zk_input_")]
    if not zk_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ZKè¾“å…¥æ–‡ä»¶")
        return False
    
    # ä»æ–‡ä»¶åæå–buy_hash
    first_file = zk_files[0]
    buy_hash_prefix = first_file.split('_')[2]  # zk_input_{å‰ç¼€}_{å›¾ç‰‡å}.json
    
    # æ„é€ å®Œæ•´çš„buy_hashï¼ˆè¿™é‡Œç”¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„ï¼Œå®é™…åº”è¯¥ä»watermark_info.jsonè·å–ï¼‰
    print(f"ğŸ“‹ æ£€æµ‹åˆ°buy_hashå‰ç¼€: {buy_hash_prefix}")
    print(f"ğŸ“‹ æ‰¾åˆ° {len(zk_files)} ä¸ªZKè¾“å…¥æ–‡ä»¶")
    
    # æ„é€ æµ‹è¯•ç”¨çš„buy_hashï¼ˆå®é™…åº”è¯¥ä»watermark_info.jsonè¯»å–ï¼‰
    test_buy_hash = f"{buy_hash_prefix}{'0' * (64 - len(buy_hash_prefix))}"  # å¡«å……åˆ°64ä½
    
    # ä»watermark_info.jsonè·å–çœŸå®çš„buy_hash
    watermark_info_file = os.path.join(data_dir, "watermark_info.json")
    if os.path.exists(watermark_info_file):
        with open(watermark_info_file, 'r') as f:
            watermark_info = json.load(f)
        
        # æŸ¥æ‰¾åŒ¹é…çš„è®°å½•
        if "records" in watermark_info:
            for record in watermark_info["records"]:
                if record.get("buy_hash", "").startswith(buy_hash_prefix):
                    test_buy_hash = record["buy_hash"]
                    print(f"âœ… ä»watermark_info.jsonè·å–å®Œæ•´buy_hash: {test_buy_hash[:16]}...")
                    break
    
    # 2. éªŒè¯ZKè¾“å…¥æ–‡ä»¶æŸ¥æ‰¾åŠŸèƒ½
    print(f"\n1. æµ‹è¯•ZKè¾“å…¥æ–‡ä»¶æŸ¥æ‰¾...")
    zk_result = find_zk_input_for_buy_hash(test_buy_hash, data_dir)
    
    if not zk_result:
        print(f"âŒ æ— æ³•æ‰¾åˆ°buy_hashå¯¹åº”çš„ZKè¾“å…¥æ–‡ä»¶")
        return False
    
    print(f"âœ… æ‰¾åˆ° {zk_result['total_files']} ä¸ªZKè¾“å…¥æ–‡ä»¶:")
    for i, file_info in enumerate(zk_result['zk_input_files'], 1):
        print(f"   {i}. {file_info['filename']} ({file_info['file_size']/1024:.1f} KB)")
    
    # 3. æµ‹è¯•ä¸åŒçš„åˆ†å—å¤§å°
    test_chunk_sizes = [
        {"name": "è¶…å¤§åˆ†å—", "m": 10000, "power": 20},  # å•åˆ†å—
        {"name": "ä¸­ç­‰åˆ†å—", "m": 5000, "power": 19},   # 2åˆ†å—
        {"name": "å°åˆ†å—", "m": 2000, "power": 18},     # 5åˆ†å—
    ]
    
    for i, test_case in enumerate(test_chunk_sizes, 1):
        print(f"\n{i+1}. æµ‹è¯•{test_case['name']} (m={test_case['m']})...")
        
        optimal_config = {
            "power": test_case["power"],
            "constraint_size": 2 ** test_case["power"],
            "M": (9216 + test_case["m"] - 1) // test_case["m"],  # åŸºäº96x96=9216åƒç´ è®¡ç®—
            "m": test_case["m"],
            "total_time": 120.0
        }
        
        print(f"   é…ç½®: 2^{optimal_config['power']} çº¦æŸ, {optimal_config['M']} åˆ†å—, {optimal_config['m']} åƒç´ /å—")
        
        # æ‰§è¡Œåˆ†å—å¤„ç†
        result = process_watermarked_dataset_registration(test_buy_hash, optimal_config)
        
        if result["status"] == "chunking_completed":
            chunking_result = result["chunking_result"]
            session_info = chunking_result["session_info"]
            
            print(f"   âœ… åˆ†å—æˆåŠŸ!")
            print(f"      - å¤„ç†å›¾ç‰‡æ•°: {session_info['total_images']}")
            print(f"      - ç”Ÿæˆåˆ†å—æ•°: {session_info['total_chunks']}")
            print(f"      - è¾“å‡ºç›®å½•: {os.path.basename(session_info['output_directory'])}")
            
            # éªŒè¯åˆ†å—æ–‡ä»¶
            chunk_output_dir = chunking_result["chunk_output_dir"]
            if os.path.exists(chunk_output_dir):
                total_files = sum(len(files) for _, _, files in os.walk(chunk_output_dir))
                print(f"      - éªŒè¯æ–‡ä»¶æ•°: {total_files} ä¸ªJSONæ–‡ä»¶")
                
                # æ£€æŸ¥ç¬¬ä¸€ä¸ªåˆ†å—æ–‡ä»¶çš„æ ¼å¼
                for root, dirs, files in os.walk(chunk_output_dir):
                    if files:
                        first_chunk = os.path.join(root, files[0])
                        try:
                            with open(first_chunk, 'r') as f:
                                chunk_data = json.load(f)
                            
                            required_keys = ["originalPixelValues", "Watermark_PixelValues", 
                                           "binaryWatermark", "binaryWatermark_num"]
                            has_all_keys = all(key in chunk_data for key in required_keys)
                            
                            if has_all_keys:
                                pixel_count = len(chunk_data["originalPixelValues"])
                                print(f"      âœ… æ ¼å¼æ­£ç¡®ï¼Œåƒç´ æ•°: {pixel_count}")
                            else:
                                print(f"      âŒ æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘å¿…è¦å­—æ®µ")
                        except Exception as e:
                            print(f"      âŒ è¯»å–åˆ†å—æ–‡ä»¶å¤±è´¥: {e}")
                        break
            else:
                print(f"      âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
        else:
            print(f"   âŒ åˆ†å—å¤±è´¥: {result.get('message', 'Unknown error')}")
            return False
    
    print(f"\nğŸ‰ ç¬¬ä¸‰é˜¶æ®µåˆ†å—æµ‹è¯•å®Œæˆ!")
    print(f"âœ… æ‰€æœ‰åˆ†å—å¤§å°æµ‹è¯•é€šè¿‡")
    print(f"âœ… åˆ†å—æ ¼å¼ç¬¦åˆLSB_groth16è¦æ±‚")
    print(f"âœ… æ–‡ä»¶ç»„ç»‡ç»“æ„æ­£ç¡®")
    
    return True

def verify_chunk_format_compatibility():
    """éªŒè¯åˆ†å—æ ¼å¼ä¸LSB_groth16çš„å…¼å®¹æ€§"""
    print(f"\n=== åˆ†å—æ ¼å¼å…¼å®¹æ€§éªŒè¯ ===")
    
    # æŸ¥æ‰¾æœ€æ–°ç”Ÿæˆçš„åˆ†å—æ–‡ä»¶
    data_dir = os.path.join(os.path.dirname(__file__), "data")
    chunked_inputs_dir = os.path.join(data_dir, "chunked_inputs")
    
    if not os.path.exists(chunked_inputs_dir):
        print(f"âŒ åˆ†å—è¾“å…¥ç›®å½•ä¸å­˜åœ¨")
        return False
    
    # æŸ¥æ‰¾æœ€æ–°çš„ä¼šè¯ç›®å½•
    session_dirs = [d for d in os.listdir(chunked_inputs_dir) if d.startswith("session_")]
    if not session_dirs:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åˆ†å—ä¼šè¯ç›®å½•")
        return False
    
    latest_session = max(session_dirs, 
                        key=lambda d: os.path.getmtime(os.path.join(chunked_inputs_dir, d)))
    
    session_path = os.path.join(chunked_inputs_dir, latest_session)
    print(f"ğŸ“ éªŒè¯ä¼šè¯: {latest_session}")
    
    # æŸ¥æ‰¾åˆ†å—ç›®å½•
    chunk_dirs = [d for d in os.listdir(session_path) if d.startswith("chunk_pixel_")]
    if not chunk_dirs:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åˆ†å—ç›®å½•")
        return False
    
    # éªŒè¯æ¯ä¸ªåˆ†å—ç›®å½•
    for chunk_dir in chunk_dirs:
        chunk_path = os.path.join(session_path, chunk_dir)
        print(f"\nğŸ“‹ éªŒè¯åˆ†å—ç›®å½•: {chunk_dir}")
        
        # éå†å›¾ç‰‡ç›®å½•
        image_dirs = [d for d in os.listdir(chunk_path) if d.startswith("input_")]
        print(f"   æ‰¾åˆ° {len(image_dirs)} ä¸ªå›¾ç‰‡ç›®å½•")
        
        for image_dir in image_dirs:
            image_path = os.path.join(chunk_path, image_dir)
            chunk_files = [f for f in os.listdir(image_path) if f.endswith('.json')]
            
            print(f"   {image_dir}: {len(chunk_files)} ä¸ªåˆ†å—æ–‡ä»¶")
            
            # éªŒè¯ç¬¬ä¸€ä¸ªåˆ†å—æ–‡ä»¶çš„è¯¦ç»†æ ¼å¼
            if chunk_files:
                first_chunk_file = os.path.join(image_path, chunk_files[0])
                try:
                    with open(first_chunk_file, 'r') as f:
                        chunk_data = json.load(f)
                    
                    # æ£€æŸ¥å­—æ®µåç§°ï¼ˆå¿…é¡»ä¸LSB_groth16ä¸€è‡´ï¼‰
                    expected_fields = {
                        "originalPixelValues": "åŸå§‹åƒç´ å€¼æ•°ç»„",
                        "Watermark_PixelValues": "æ°´å°åƒç´ å€¼æ•°ç»„", 
                        "binaryWatermark": "äºŒè¿›åˆ¶æ°´å°æ•°ç»„",
                        "binaryWatermark_num": "æ°´å°é•¿åº¦å­—ç¬¦ä¸²"
                    }
                    
                    format_correct = True
                    for field, description in expected_fields.items():
                        if field not in chunk_data:
                            print(f"      âŒ ç¼ºå°‘å­—æ®µ: {field} ({description})")
                            format_correct = False
                        else:
                            print(f"      âœ… {field}: {type(chunk_data[field])}")
                    
                    if format_correct:
                        # éªŒè¯æ•°æ®ç±»å‹å’Œç»“æ„
                        ori_pixels = chunk_data["originalPixelValues"]
                        wm_pixels = chunk_data["Watermark_PixelValues"] 
                        binary_wm = chunk_data["binaryWatermark"]
                        wm_num = chunk_data["binaryWatermark_num"]
                        
                        print(f"      ğŸ“Š æ•°æ®éªŒè¯:")
                        print(f"         åŸå§‹åƒç´ : {len(ori_pixels)} ä¸ªRGBä¸‰å…ƒç»„")
                        print(f"         æ°´å°åƒç´ : {len(wm_pixels)} ä¸ªRGBä¸‰å…ƒç»„")
                        print(f"         äºŒè¿›åˆ¶æ°´å°: {len(binary_wm)} ä½")
                        print(f"         æ°´å°é•¿åº¦: {wm_num}")
                        
                        # éªŒè¯åƒç´ æ ¼å¼
                        if ori_pixels and isinstance(ori_pixels[0], list) and len(ori_pixels[0]) == 3:
                            print(f"      âœ… åƒç´ æ ¼å¼æ­£ç¡®: RGBä¸‰å…ƒç»„")
                        else:
                            print(f"      âŒ åƒç´ æ ¼å¼é”™è¯¯")
                            format_correct = False
                        
                        # éªŒè¯äºŒè¿›åˆ¶æ°´å°æ ¼å¼
                        if all(bit in [0, 1] for bit in binary_wm[:100]):  # æ£€æŸ¥å‰100ä½
                            print(f"      âœ… äºŒè¿›åˆ¶æ°´å°æ ¼å¼æ­£ç¡®: 0/1æ•´æ•°")
                        else:
                            print(f"      âŒ äºŒè¿›åˆ¶æ°´å°æ ¼å¼é”™è¯¯")
                            format_correct = False
                        
                        if format_correct:
                            print(f"      ğŸ‰ æ ¼å¼å®Œå…¨å…¼å®¹LSB_groth16!")
                        
                except Exception as e:
                    print(f"      âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
    
    print("ğŸ§ª ç¬¬ä¸‰é˜¶æ®µå®Œæ•´æµ‹è¯•")
    print("=" * 60)
    
    success = True
    
    # æµ‹è¯•1: åˆ†å—åŠŸèƒ½
    try:
        if not test_stage3_chunking():
            success = False
    except Exception as e:
        print(f"âŒ åˆ†å—æµ‹è¯•å¤±è´¥: {e}")
        success = False
    
    # æµ‹è¯•2: æ ¼å¼å…¼å®¹æ€§
    try:
        if not verify_chunk_format_compatibility():
            success = False
    except Exception as e:
        print(f"âŒ æ ¼å¼éªŒè¯å¤±è´¥: {e}")
        success = False
    
    # æ€»ç»“
    print(f"\n" + "=" * 60)
    if success:
        print(f"ğŸ‰ ç¬¬ä¸‰é˜¶æ®µæµ‹è¯•å…¨éƒ¨é€šè¿‡!")
        print(f"âœ… æ•°æ®åˆ†å—åŠŸèƒ½æ­£å¸¸")
        print(f"âœ… æ ¼å¼å®Œå…¨å…¼å®¹LSB_groth16")
        print(f"âœ… ä½¿ç”¨ç™»è®°é˜¶æ®µçš„çº¦æŸå‚æ•°")
        print(f"ğŸ“‹ å¯ä»¥è¿›å…¥ç¬¬å››é˜¶æ®µï¼šé›¶çŸ¥è¯†è¯æ˜ç”Ÿæˆ")
    else:
        print(f"âŒ ç¬¬ä¸‰é˜¶æ®µæµ‹è¯•éƒ¨åˆ†å¤±è´¥")
        print(f"éœ€è¦ä¿®å¤å¤±è´¥é¡¹ç›®åå†ç»§ç»­")
    
    return success

if __name__ == "__main__":
    main() 
 